{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "utSjrT6RsX7n"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import utm\n",
    "import random\n",
    "import Transformer as tr\n",
    "import preprocess as pr\n",
    "import simpleTrajVisualizer as vis\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_data_file_path = \"\"  ## path to trajectory dataset\n",
    "trained_model_file_path = \"\"    ## path to save trained model \n",
    "scalar_file_path = \"\"           ## path to save fitted scalar (leave empty if none)\n",
    "\n",
    "number_of_bins = 30*30          ## number of bins\n",
    "lr = 0.1                        ## learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CDLhAOXDdoU3"
   },
   "outputs": [],
   "source": [
    "traj_data = pd.read_csv(trajectory_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iW2pPb1o5-hJ"
   },
   "outputs": [],
   "source": [
    "train_data_inputs, test_data_inputs, train_data_targets, test_data_targets, train_indx, test_indx = pr.preprocess_dataset(traj_data, max = 100, max_len = 50, input_len = 20, False, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjKmqw9F5lND",
    "outputId": "40fe8f19-521c-4678-e5e9-7bfdd572075e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10100 256\n",
      "10100 256\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  src_pad_idx = -999\n",
    "  trg_pad_idx = -999\n",
    "  model = tr.Transformer(number_of_bins, number_of_bins, src_pad_idx, trg_pad_idx, device=device, dropout=0.15).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "id": "YL0Uoilr5HId",
    "outputId": "06f3e0ce-6973-4a40-e6aa-62050d02ac2b"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "import math\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    for batch, i in enumerate(range(0, train_data_inputs.size(0), 1)):\n",
    "      data = train_data_inputs[i,:,:]\n",
    "      targets = train_data_targets[i,:,:]\n",
    "      optimizer.zero_grad()\n",
    "      output = model(data, targets[:, :])\n",
    "      loss = criterion(output[0,:,:], targets[0,:])\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "      optimizer.step()\n",
    "      total_loss += loss.item()\n",
    "        \n",
    "    cur_loss = total_loss / train_data_inputs.size(0)\n",
    "    print('lr {:02.2f} | '\n",
    "                'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                  scheduler.get_lr()[0],\n",
    "                  cur_loss, math.exp(cur_loss)))\n",
    "\n",
    "def evaluate(eval_model, data_source_inputs, data_source_targets):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source_inputs.size(0), 1):\n",
    "            # data, targets = get_batch(data_source, i)\n",
    "            data = data_source_inputs[i,:,:]\n",
    "            targets = data_source_targets[i,:,:]\n",
    "            output = eval_model(data, targets[:, :])\n",
    "            output_flat = output\n",
    "            total_loss += 1 * criterion(output[0,:,:], targets[0,:]).item()\n",
    "\n",
    "    print(\"loss: \", total_loss / (data_source_inputs.size(0)))\n",
    "    return total_loss / (data_source_inputs.size(0))\n",
    "\n",
    "\n",
    "def get_predictions(eval_model, data_source_inputs, data_source_targets):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    preds_list = []\n",
    "    target_list = []\n",
    "    inputs_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source_inputs.size(0), 1):\n",
    "            data = data_source_inputs[i,:,:]\n",
    "            targets = data_source_targets[i,:,:]\n",
    "            output = eval_model(data, targets[:, :])\n",
    "            preds_list.append(torch.argmax(output[0,:,:], dim=1).tolist())\n",
    "            target_list.append(targets[0,:].tolist())\n",
    "            inputs_list.append(data[0,:].tolist())\n",
    "\n",
    "    return inputs_list, target_list, preds_list\n",
    "\n",
    "def displacement_error(x_diff, y_diff):\n",
    "    squared_dist = np.linalg.norm((x_diff,y_diff))**2\n",
    "    return squared_dist\n",
    "\n",
    "def calculate_performance_metrics(targets, preds):\n",
    "  x_diff = []\n",
    "  y_diff = []\n",
    "  \n",
    "  for indx in range(0, len(targets)):\n",
    "    yt = targets[indx] % 100\n",
    "    xt = targets[indx] / 100\n",
    "\n",
    "    yp = preds[indx] % 100\n",
    "    xp = preds[indx] / 100\n",
    "\n",
    "    tar_utm = utm.from_latlon(yt, xt)\n",
    "    pred_utm = utm.from_latlon(yp, xp)\n",
    "\n",
    "    dist_x = tar_utm[0] - pred_utm[0]\n",
    "    dist_y = tar_utm[1] - pred_utm[1]\n",
    "\n",
    "    x_diff.append(dist_x)\n",
    "    y_diff.append(dist_y)\n",
    "\n",
    "  \n",
    "  dict_pd = {'X_DIFF': x_diff, 'Y_DIFF': y_diff} \n",
    "  output_df = pd.DataFrame(dict_pd)\n",
    "  output_df[['X_DIFF', 'Y_DIFF']] = scaler.inverse_transform(output_df[['X_DIFF', 'Y_DIFF']])\n",
    "  output_df['DIST'] = output_df.apply(lambda row: displacement_error(row['X_DIFF'], row['Y_DIFF']), axis=1)  \n",
    "  ADE = output_df['DIST'].mean()\n",
    "  FDE = output_df.tail(1)['DIST'].item()\n",
    "  \n",
    "  return (ADE, FDE) ## Average Displacement Error ADE, Final Displacement Error (FDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HQN0OkjK5s9s"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b2bbfa12b5be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mepoch_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m   \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m   \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m89\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-9f4851dd6c8a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m       \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m       \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\transformers\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\transformers\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 100 # The number of epochs\n",
    "best_model = None\n",
    "early_stopping = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "  epoch_start_time = time.time()\n",
    "  train()\n",
    "  val_loss = evaluate(model, test_data_inputs, test_data_targets)\n",
    "  print('-' * 89)\n",
    "  print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                    val_loss, math.exp(val_loss)))\n",
    "  print('-' * 89)\n",
    "\n",
    "  early_stopping = early_stopping + 1\n",
    "\n",
    "  if val_loss < best_val_loss:\n",
    "    early_stopping = 0\n",
    "    best_val_loss = val_loss\n",
    "    best_model = model\n",
    "\n",
    "  if early_stopping > 3:\n",
    "    break\n",
    "\n",
    "  scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_data_inputs, test_data_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPUn4MMB55ES"
   },
   "outputs": [],
   "source": [
    "torch.save(model, trained_model_file_path) ## save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(scaler, scalar_file_path)  ## save fitted scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZNzdAbBY1gB"
   },
   "outputs": [],
   "source": [
    "vis.vis_trajectory_scatter(best_model, test_data_inputs, test_data_targets) ## simple visualization of trajectories"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Train_Transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
